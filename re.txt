domanda 1:
	definire manualmente più classificatori sul file "manuale.csv", adottando almeno due dei
	modelli illustrati a lezione, e valutando le prestazioni da loro ottenute sullo stesso file
	“manuale.csv”;
		algoritmi utilizzati:
			
			-KNN:
				Il K-Nearest Neighbors (KNN) è un algoritmo di classificazione e regressione molto semplice, ma potente, utilizzato principalmente per problemi di classificazione. È un algoritmo basato su istanze, il che significa che non apprende esplicitamente un modello dai dati, ma memorizza semplicemente l’intero dataset di addestramento e fa previsioni in base alla distanza tra i punti. Vediamo in dettaglio come funziona e come applicarlo.

				Come funziona il KNN?
					Il principio di base del KNN è che per fare una previsione su un nuovo campione (punto), si cercano i  k  campioni più vicini nel dataset di addestramento, cioè i vicini più vicini, e si assegna al campione la classe o il valore più comune tra questi vicini.

				Passi principali:
					1.	Scegli il numero di vicini  k : Questo è un parametro dell’algoritmo, che indica quanti vicini considerare. Ad esempio, se  k = 3 , il nuovo campione verrà classificato considerando le 3 istanze più vicine a esso nel dataset di addestramento.

					2.	Calcola la distanza: Per ciascun punto nel dataset di addestramento, calcola la distanza rispetto al nuovo campione da classificare. Spesso si usa la distanza euclidea, ma altre misure come la distanza di Manhattan o Minkowski possono essere utilizzate.
					La distanza euclidea tra due punti  A  e  B  in uno spazio n-dimensionale è data dalla formula:

						d(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}

					3.	Trova i  k  vicini più vicini: Una volta calcolata la distanza di ogni punto del dataset di addestramento rispetto al nuovo campione, ordina i punti in base alla distanza e seleziona i  k  più vicini.

					4.	Fai una previsione: Se il problema è di classificazione, assegna al nuovo campione la classe più frequente tra i  k  vicini più vicini.

				Punti chiave del KNN
					Scelta di  k :
						Un valore piccolo di  k  rende il modello più sensibile ai dati rumorosi, poiché può basarsi solo su un vicino molto vicino, che potrebbe essere un outlier.
						Un valore troppo grande di  k  può portare a un modello che non riesce a cogliere la vera struttura del dataset, poiché tiene conto di molti vicini lontani.
						Spesso  k  viene scelto con la cross-validation

			-GAUSSIAN NAIVE BAYES:
				Il Gaussian Naive Bayes si basa sul teorema di Bayes, che descrive la probabilità di una classe  C  dato un set di dati  X :

				P(C|X) = [P(X|C) P(C)]/P(X)

				Dove:
					-P(C|X):probabilità a posteriori, cioè la probabilità che un esempio appartenga alla classe  C  dato il vettore di feature  X.
					-P(X|C):probabilità che  X  si verifichi dato che la classe è  C  (probabilità condizionata).
					-P(C):probabilità a priori della classe  C.
					-P(X):probabilità marginale di  X  (può essere ignorata per la classificazione, perché è costante per tutte le classi).

				-Assunzione “Naive”:

				L’algoritmo assume che le feature di  X = [x_1, x_2, ..., x_n]  siano indipendenti condizionatamente alla classe  C . Questo semplifica molto i calcoli, poiché:

				P(X|C) = P(x_1|C) * P(x_2|C) ... P(x_n|C)

				In altre parole, si assume che ogni feature contribuisca indipendentemente alla probabilità complessiva, dato che la classe è  C.

				Il Gaussian Naive Bayes assume che ogni feature  x_i  (dato C) sia distribuita secondo una distribuzione normale, la distribuzione normale descrive la probabilità che un valore  x_i  si presenti in una certa classe.

				per classificare un esempio:
				-calcoliamo P(C), la probabilità a priori della classe C;
				-calcoliamo P(X|C), usando la formula della distribuzione normale per ogni feature x_i;
				-combiniamo queste probabilità per ogni classe usando il teorema di Bayes;
				-assegniamo l'etichetta della classe con la probabilità a posteriari più alta;

				Il Gaussian Naive Bayes è un metodo molto efficace per la classificazione quando le feature numeriche seguono una distribuzione normale. Nonostante le sue assunzioni semplificative, può essere sorprendentemente preciso e rimane un punto di partenza utile per molti problemi di machine learning.


			-PERCETTRONE MULTI-CLASSE:
				Il percettrone multi-classe è una generalizzazione del percettrone classico per gestire problemi di classificazione con più di due classi. A differenza del percettrone binario, che separa solo due classi con un singolo iperpiano, il percettrone multi-classe utilizza un iperpiano per ogni classe e assegna l’etichetta in base al punteggio (o attivazione) massimo. Vediamo come funziona in dettaglio:

				Struttura del Percettrone Multi-Classe:
				Supponiamo di avere:
				-k  classi:  C_1, C_2, ..., C_k .
				-Un vettore di input  x = [x_1, x_2, ..., x_n]  con  n  feature.

				Ogni classe  C_i  ha:
					-Un vettore di pesi  w_i = [w_{i1}, w_{i2}, ..., w_{in}] .
					-Un valore di bias  b_i .

				La somma ponderata (attivazione) per la classe  i  è:

				z_i = w_i^T*x + b_i

				Dove  w_i^T*x  è il prodotto scalare tra il vettore di pesi  w_i  e l’input  x.

				Funzione di Decisione:
				La classe assegnata al vettore di input  x  è quella con il punteggio  z_i  più alto:

				y^ = \text{argmax}_i (z_i), dove y^ è la classe predetta e la funzione argmax seleziona l'indice i che corrisponde al massimo valore di z_i.

				Addestramento del Percettrone Multi-Classe:
				L’obiettivo è imparare i pesi  w_i  e i bias  b_i  per ciascuna classe in modo che i dati siano classificati correttamente. L’addestramento segue un algoritmo iterativo basato sulla regola del percettrone.
				algoritmo: 
				-i pesi w_i e b_i vengono inizializzati (spesso a zero con piccoli valori casuali);
				-per ogni esempio di training (x,y), dove x è il vettore di input e y è l'etichetta della classe corretta;
				-calcoliamo z_i per ogni classe i;
				-determiniamo la classe predetta y^=argmax_i(z_i);
				-aggiorniamo i pesi per la classe predetta (solo se y^ diversa da y)[AGGIORNAMENTO DEI PESI]
					-per favorirla oppure per penalizzarla;
				-l'algoritmo si ripete per tutti i dati di training fino a che:
					-tutti i dati sono classificati correttamente 
					-oppure viene raggiunto il numero massimo di iterazioni


		dubbi:
			il dataset va ripulito?
			va splittato per training e testing?     

domanda 2:
	verificare che il dataset “training.csv” non contenga osservazioni palesemente errate ed
	effettuare l’analisi esplorativa del dataset rappresentando i risultati anche in forma grafica
	(boxplot e/o pairplot e matrice di correlazione);
		alcuni controlli da fare sul dataset:
		-Verificare la Formattazione dei Dati:
			Controlla che i dati rispettino il formato atteso. Ad esempio, assicurati che i numeri siano effettivamente numeri, le date siano nel formato corretto e le stringhe siano coerenti. Se trovi valori strani, come una data con un mese “13” o un valore numerico inserito come testo, questi potrebbero indicare errori di registrazione.
		-Individuare i Valori Mancanti:
			Un passo importante è identificare se ci sono valori mancanti in alcune colonne o righe. Ad esempio, se la colonna dell’età ha molti valori vuoti, dovresti chiederti se è un errore o se è accettabile in quel contesto. Spesso i valori mancanti possono essere un sintomo di un problema nel processo di raccolta dati.
		-Cercare Valori Anomali o Implausibili:
			Esamina ogni variabile per verificare che i valori siano plausibili. Per i numeri, ad esempio, controlla che rientrino in un intervallo logico: un’età negativa o superiore a 120 è chiaramente errata. Per i dati categoriali, come il genere o le regioni, verifica che tutte le voci appartengano a un elenco di categorie valide. Errori di battitura o formati diversi (es. “male”, “Male”, “M”) possono compromettere l’analisi.
		-Esaminare la Distribuzione dei Dati:
			Analizza la distribuzione dei dati per capire se ci sono valori fuori posto. Ad esempio, un picco inaspettato in una distribuzione potrebbe segnalare un errore di registrazione o un caso estremo da indagare. Puoi anche confrontare la distribuzione con aspettative note per individuare anomalie.
		analisi esplorativa
		L’analisi esplorativa dei dati (EDA, Exploratory Data Analysis) è il processo iniziale per comprendere e analizzare un dataset in modo approfondito prima di applicare algoritmi di machine learning o modelli statistici. Lo scopo principale è identificare le caratteristiche, i pattern e le anomalie nei dati, valutandone la qualità e l’idoneità per il problema da risolvere.
		rappresentazione dei risultati dell'analisi esplorativa del dataset:

		1.Boxplot
		Il boxplot è uno strumento grafico che rappresenta la distribuzione di una variabile numerica e mette in evidenza i principali parametri statistici, come la mediana, i quartili e gli outlier. È ideale per individuare distribuzioni e valori anomali.
		Elementi del Boxplot:

		•Box centrale: Indica il 50% centrale dei dati (tra il primo quartile, Q1, e il terzo quartile, Q3).
		•La linea al centro del box rappresenta la mediana.
		•“Baffi” (whiskers): Estendono fino a un limite calcolato come  Q1 - 1.5 \times IQR  o  Q3 + 1.5 \times IQR , dove 
		 IQR  è l’intervallo interquartile (Q3 - Q1).
		•Outlier: Punti fuori dai “baffi”, che rappresentano valori anomali.

		Quando usare il Boxplot:
		•Per confrontare la distribuzione di una variabile numerica tra diverse categorie.
		•Per individuare outlier.
		•Per valutare la simmetria o l’asimmetria della distribuzione.

		2.Pairplot 
		Il Pairplot è una rappresentazione visiva delle relazioni tra tutte le variabili numeriche di un dataset. Mostra una matrice di scatterplot per ogni coppia di variabile e un istogramma o KDE(Kernel Density Estimation) lungo la diagonale per rappresentare la distribuzione di ogni 
		singola variabile.

		Caratteristiche principali:
			•Ogni cella nella matrice rappresenta un grafico che mette a confronto due variabili.
			•La diagonale include grafici univariati per mostrare la distribuzione delle variabili.
			•Se il dataset ha variabili categoriche, i punti possono essere colorati per evidenziare gruppi diversi.

		Quando usare il Pairplot:
			•Per esplorare relazioni tra variabili numeriche (es. correlazioni, pattern, distribuzioni).
			•Per individuare cluster o anomalie.
			•Per analizzare simultaneamente più coppie di variabili.

		3.Matrice di correlazione 
		La matrice di correlazione misura e visualizza la relazione lineare tra le variabili numeriche di un dataset.
		Ogni cella della matrice contiene il coefficiente di correlazione (che va da -1 a 1).
		Significato dei valori:
			•1: Correlazione perfetta positiva (se una variabile aumenta, l’altra aumenta).
			•-1: Correlazione perfetta negativa (se una variabile aumenta, l’altra diminuisce).
			•0: Nessuna correlazione (le variabili sono indipendenti linearmente).

		Visualizzazione:
		La matrice è spesso rappresentata con una heatmap, dove:
			•I colori (dal rosso al blu, ad esempio) indicano la forza e la direzione della correlazione.
			•Possono essere annotate le celle con i valori di correlazione.

		Quando usare la Matrice di Correlazione:

			•Per identificare variabili fortemente correlate (che potrebbero creare multicollinearità nei modelli).
			•Per individuare relazioni utili per la selezione delle feature.
			•Per escludere variabili ridondanti.

domanda 3:
	implementare i classificatori progettati al punto 1) in Python e valutare le performance
	ottenute da ognuno di essi sul file "training.csv" o su qualche suo sottoinsieme, cercando di
	ottimizzare le prestazioni dei classificatori;

domanda 4:
	con riferimento al file “training.csv”, addestrare tramite Scikit-Learn più classificatori,
	separando opportunamente i campioni nel training set e nel test set, con l'obiettivo di
	massimizzare le prestazioni sul test set. Alla fine della fase di addestramento, selezionare il
	classificatore ritenuto più performante. In sede d'esame sarà fornito un altro file,
	denominato "real_settings.csv", mirato a testare le prestazioni di tale classificatore.

	