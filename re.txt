algoritmi che possiamo utilizzare sul manuale.csv:
	-alberi decisionali 
	-KNN:
		Il K-Nearest Neighbors (KNN) è un algoritmo di classificazione e regressione molto semplice, ma potente, utilizzato principalmente per problemi di classificazione. È un algoritmo basato su istanze, il che significa che non apprende esplicitamente un modello dai dati, ma memorizza semplicemente l’intero dataset di addestramento e fa previsioni in base alla distanza tra i punti. Vediamo in dettaglio come funziona e come applicarlo.

		Come funziona il KNN?
			Il principio di base del KNN è che per fare una previsione su un nuovo campione (punto), si cercano i  k  campioni più vicini nel dataset di addestramento, cioè i vicini più vicini, e si assegna al campione la classe o il valore più comune tra questi vicini.

		Passi principali:
			1.	Scegli il numero di vicini  k : Questo è un parametro dell’algoritmo, che indica quanti vicini considerare. Ad esempio, se  k = 3 , il nuovo campione verrà classificato considerando le 3 istanze più vicine a esso nel dataset di addestramento.

			2.	Calcola la distanza: Per ciascun punto nel dataset di addestramento, calcola la distanza rispetto al nuovo campione da classificare. Spesso si usa la distanza euclidea, ma altre misure come la distanza di Manhattan o Minkowski possono essere utilizzate.
			La distanza euclidea tra due punti  A  e  B  in uno spazio n-dimensionale è data dalla formula:

				d(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}

			3.	Trova i  k  vicini più vicini: Una volta calcolata la distanza di ogni punto del dataset di addestramento rispetto al nuovo campione, ordina i punti in base alla distanza e seleziona i  k  più vicini.

			4.	Fai una previsione: Se il problema è di classificazione, assegna al nuovo campione la classe più frequente tra i  k  vicini più vicini.

		Punti chiave del KNN
			Scelta di  k :
				Un valore piccolo di  k  rende il modello più sensibile ai dati rumorosi, poiché può basarsi solo su un vicino molto vicino, che potrebbe essere un outlier.
				Un valore troppo grande di  k  può portare a un modello che non riesce a cogliere la vera struttura del dataset, poiché tiene conto di molti vicini lontani.
				Spesso  k  viene scelto con la cross-validation

	-GAUSSIAN NAIVE BAYES:
		Il Gaussian Naive Bayes si basa sul teorema di Bayes, che descrive la probabilità di una classe  C  dato un set di dati  X :

		P(C|X) = [P(X|C) P(C)]/P(X)

		Dove:
			-P(C|X):probabilità a posteriori, cioè la probabilità che un esempio appartenga alla classe  C  dato il vettore di feature  X.
			-P(X|C):probabilità che  X  si verifichi dato che la classe è  C  (probabilità condizionata).
			-P(C):probabilità a priori della classe  C.
			-P(X):probabilità marginale di  X  (può essere ignorata per la classificazione, perché è costante per tutte le classi).

		-Assunzione “Naive”:

		L’algoritmo assume che le feature di  X = [x_1, x_2, ..., x_n]  siano indipendenti condizionatamente alla classe  C . Questo semplifica molto i calcoli, poiché:

		P(X|C) = P(x_1|C) * P(x_2|C) ... P(x_n|C)

		In altre parole, si assume che ogni feature contribuisca indipendentemente alla probabilità complessiva, dato che la classe è  C.

		Il Gaussian Naive Bayes assume che ogni feature  x_i  (dato C) sia distribuita secondo una distribuzione normale, la distribuzione normale descrive la probabilità che un valore  x_i  si presenti in una certa classe.

		per classificare un esempio:
		-calcoliamo P(C), la probabilità a priori della classe C;
		-calcoliamo P(X|C), usando la formula della distribuzione normale per ogni feature x_i;
		-combiniamo queste probabilità per ogni classe usando il teorema di Bayes;
		-assegniamo l'etichetta della classe con la probabilità a posteriari più alta;

		Il Gaussian Naive Bayes è un metodo molto efficace per la classificazione quando le feature numeriche seguono una distribuzione normale. Nonostante le sue assunzioni semplificative, può essere sorprendentemente preciso e rimane un punto di partenza utile per molti problemi di machine learning.


	-PERCETTRONE MULTI-CLASSE:
		Il percettrone multi-classe è una generalizzazione del percettrone classico per gestire problemi di classificazione con più di due classi. A differenza del percettrone binario, che separa solo due classi con un singolo iperpiano, il percettrone multi-classe utilizza un iperpiano per ogni classe e assegna l’etichetta in base al punteggio (o attivazione) massimo. Vediamo come funziona in dettaglio:

		Struttura del Percettrone Multi-Classe:
		Supponiamo di avere:
		-k  classi:  C_1, C_2, ..., C_k .
		-Un vettore di input  x = [x_1, x_2, ..., x_n]  con  n  feature.

		Ogni classe  C_i  ha:
			-Un vettore di pesi  w_i = [w_{i1}, w_{i2}, ..., w_{in}] .
			-Un valore di bias  b_i .

		La somma ponderata (attivazione) per la classe  i  è:

		z_i = w_i^T*x + b_i

		Dove  w_i^T*x  è il prodotto scalare tra il vettore di pesi  w_i  e l’input  x.

		Funzione di Decisione:
		La classe assegnata al vettore di input  x  è quella con il punteggio  z_i  più alto:

		y^ = \text{argmax}_i (z_i), dove y^ è la classe predetta e la funzione argmax seleziona l'indice i che corrisponde al massimo valore di z_i.

		Addestramento del Percettrone Multi-Classe:
		L’obiettivo è imparare i pesi  w_i  e i bias  b_i  per ciascuna classe in modo che i dati siano classificati correttamente. L’addestramento segue un algoritmo iterativo basato sulla regola del percettrone.
		algoritmo: 
		-i pesi w_i e b_i vengono inizializzati (spesso a zero con piccoli valori casuali);
		-per ogni esempio di training (x,y), dove x è il vettore di input e y è l'etichetta della classe corretta;
		-calcoliamo z_i per ogni classe i;
		-determiniamo la classe predetta y^=argmax_i(z_i);
		-aggiorniamo i pesi per la classe predetta (solo se y^ diversa da y)[AGGIORNAMENTO DEI PESI]
			-per favorirla oppure per penalizzarla;
		-l'algoritmo si ripete per tutti i dati di training fino a che:
			-tutti i dati sono classificati correttamente 
			-oppure viene raggiunto il numero massimo di iterazioni


punto 1:
	il dataset va ripulito?
	va splittato per training e testing?      
